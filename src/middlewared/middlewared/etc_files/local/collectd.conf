<%
	import itertools
	import os
	import psutil
	import shutil
	import stat
	import subprocess
	import tarfile
	import time


	systemdataset_config = middleware.call_sync('systemdataset.config')
	is_freenas = middleware.call_sync('system.is_freenas')
	ha_node = middleware.call_sync('notifier.failover_node') if is_freenas else None
	rrd_dataset = "" if not systemdataset_config['path'] else f'{systemdataset_config["path"]}/rrd-{systemdataset_config["uuid"]}'
	if rrd_dataset or not systemdataset_config['rrd'] or (not is_freenas and middleware.call_sync('notifier.failover_status' == 'BACKUP')):
		use_rrd_dataset = True
	else:
		use_rrd_dataset = False

	# TODO: FreeNAS Config MD5 is hardcoded, this should be looked in future to see if we can change that
	# If there is a failover table remove the rc.conf cache rc.conf.local will run again using the correct collectd_enable
	# See #5019
	if is_freenas:
		try:
			os.remove('/var/tmp/freenas_config.md5')
		except FileNotFoundError:
			pass

	hostname = middleware.call_sync('system.info')['hostname']
	advanced_config = middleware.call_sync('system.advanced.config')
	graphite = advanced_config['graphite']
	cpu_in_percentage = advanced_config['cpu_in_percentage']

	rrd_file = '/data/rrd_dir.tar.bz2'
	base_dir = '/var/db/collectd'
	data_dir = '/var/db/collectd/rrd'
	rrd_mount = rrd_dataset
	# TODO: Perhaps move rrd related functionality/info to stats plugin ?
	disk_parts = psutil.disk_partitions()
	data_dir_is_ramdisk = True if [d for d in disk_parts if d.mountpoint == data_dir and d.device == 'tmpfs'] else False

	if use_rrd_dataset:
		if os.path.isdir(rrd_mount):
			if os.path.isdir(data_dir) and not os.path.islink(data_dir):
				if data_dir_is_ramdisk:
					# copy-umount-remove
					# TODO: Could using subprocess give better performance ?
					cur_time = time.strftime("%Y%m%d%H%M%S")
					shutil.copytree(data_dir, f'{data_dir}.{cur_time}', symlinks=True)
					for root, dirs, files in os.walk(data_dir):
						for o in list(itertools.chain(dirs, files)):
							st = os.stat(os.path.join(root, o))
							os.chown(os.path.join(
								f'{root.replace(data_dir, data_dir + "." + cur_time)}', o), st[stat.ST_UID], st[stat.ST_GID]
							)

					# Should we raise an exception if umount fails ?
					subprocess.Popen(
						f'umount {data_dir}', stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, close_fds=True
					).communicate()

					shutil.rmtree(data_dir)
				else:
					shutil.move(data_dir, f'{data_dir}.{time.strftime("%Y%m%d%H%M%S")}')
			# TODO: Make sure there are no issues wrt slashes
			if data_dir != rrd_mount:
				shutil.rmtree(data_dir)
				os.symlink(rrd_mount, data_dir, target_is_directory=True)
		else:
			middleware.logger.error(f'{rrd_mount} does not exist or is not a directory')
			exit(1)
	else:
		if os.path.islink(data_dir):
			shutil.rmtree(data_dir)

		os.makedirs(data_dir)

		# Create RAMdisk (if not already exists) for RRD files so they don't fill up root partition
		if not data_dir_is_ramdisk:
			subprocess.Popen(
				f'mount -t tmpfs -o size=1g tmpfs {data_dir}', stdin=subprocess.PIPE,
				stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, close_fds=True
			).communicate()

	if use_rrd_dataset:
		if not os.path.exists(rrd_mount):
			middleware.logger.error(f'{rrd_mount} does not exist')
			exit(1)
		else:
			pwd = rrd_mount + '/'
	else:
		if not os.path.exists(data_dir):
			middleware.logger.error(f'{data_dir} does not exist')
			exit(1)
		else:
			pwd = data_dir + '/'

	def get_members(tar, prefix):
		for tarinfo in tar.getmembers():
			if tarinfo.name.startswith(prefix):
				tarinfo.name = tarinfo.name[len(prefix):]
				yield tarinfo

	if os.path.isfile(rrd_file):
		with tarfile.open("test.tar") as tar:
			if 'collectd/rrd' in tar.getnames():
				tar.extractall(pwd, get_members(tar, 'collectd/rrd/'))

		if use_rrd_dataset:
			os.remove(rrd_file)

	# Migrate from old version, where "${hostname}" was a real directory
	# and "localhost" was a symlink.
	# Skip the case where "${hostname}" is "localhost", so symlink was not
	# (and is not) needed.
	if hostname != 'localhost' and os.path.isdir(pwd + hostname) and not os.path.islink(pwd + hostname):
		if os.path.isfile(f'{pwd}localhost'):
			if os.path.islink(f'{pwd}localhost'):
				os.remove(f'{pwd}localhost')
			else:
				# This should not happen, but just in case
				shutil.move(f'{pwd}localhost', f'{pwd}localhost.bak.{time.strftime("%Y%m%d%H%M%S")}')
		shutil.move(pwd + hostname, f'{pwd}localhost')

	# Remove all directories except "localhost" and it's backups (that may be erroneously created by
	# running collectd before this script)
	to_remove_dirs = [os.path.join(pwd, d) for d in os.listdir(pwd) if not d.startswith('localhost') and os.path.isdir(pwd + d)]
	for r_dir in to_remove_dirs:
		shutil.rmtree(r_dir)
	# Remove all symlinks (that are be stale if hostname was changed).
	to_remove_symlinks = [os.path.join(pwd, l) for l in os.listdir(pwd) if os.path.islink(pwd + l)]
	for r_symlink in to_remove_symlink:
		os.unlink(r_symlink)

	# Create "localhost" directory if it does not exist
	if 'localhost' not in os.listdir(pwd):
		os.makedirs(pwd + 'localhost')

	# Create "${hostname}" -> "localhost" symlink if necessary
	if hostname != 'localhost':
		os.symlink(pwd + 'localhost', pwd + hostname)

	if cpu_in_percentage:
		cpu_plugin_options = 'ValuesPercentage True'
		aggregation_plugin_cpu_type = 'percent'
	else:
		cpu_plugin_options = ''
		aggregation_plugin_cpu_type = 'cpu'

%>
Hostname "${hostname}"
FQDNLookup true
BaseDir "${base_dir}"
PIDFile "/var/run/collectd.pid"
PluginDir "/usr/local/lib/collectd"

LoadPlugin aggregation
LoadPlugin cpu
LoadPlugin cputemp
LoadPlugin ctl
LoadPlugin df
LoadPlugin disk
LoadPlugin exec
LoadPlugin geom_stat
LoadPlugin interface
LoadPlugin load
LoadPlugin memory
LoadPlugin network
LoadPlugin processes
LoadPlugin python
LoadPlugin rrdtool
LoadPlugin swap
LoadPlugin uptime
LoadPlugin syslog
LoadPlugin threshold
LoadPlugin zfs_arc
LoadPlugin zfs_arc_v2
LoadPlugin nfsstat

<Plugin "syslog">
	LogLevel err
</Plugin>

<Plugin "aggregation">
	<Aggregation>
		Plugin "cpu"
		Type "${aggregation_plugin_cpu_type}"
		GroupBy "Host"
		GroupBy "TypeInstance"
		CalculateNum true
		CalculateSum true
		CalculateAverage true
		CalculateMinimum true
		CalculateMaximum true
		CalculateStddev true
	</Aggregation>
</Plugin>
<Plugin cpu>
	${cpu_plugin_options}
</Plugin>

<Plugin cputemp>
</Plugin>

<Plugin "disk">
	Disk "/^gptid/"
	Disk "/^md/"
	Disk "/^pass/"
	IgnoreSelected true
</Plugin>

<Plugin "exec">
	NotificationExec "nobody" "/usr/local/www/freenasUI/tools/collectd_alert.py"
</Plugin>

<Plugin "interface">
	Interface "lo0"
	Interface "ipfw0"
	Interface "pflog0"
	Interface "pfsync0"
	Interface "plip0"
	Interface "/^usbus/"
	IgnoreSelected true
</Plugin>

<Plugin "rrdtool">
	DataDir "${datadir}"
% if use_rrd_dataset:
	CacheTimeout 120
	CacheFlush 900
% endif
</Plugin>

<Plugin "threshold">
	<Plugin "ctl">
		Instance "ha"
		<Type "disk_octets">
			WarningMax 10000000
			Persist true
			Interesting false
		</Type>
	</Plugin>
</Plugin>

<Plugin "zfs_arc">
</Plugin>

<Plugin "geom_stat">
	Filter "^([a]?da|ciss|md|mfi|md|nvd|pmem|xbd|vtbd)[0123456789]+$"
</Plugin>

<Plugin "df">
	Mountpoint "/"
	Mountpoint "^\/mnt(?:(?!\.zfs\/snapshot).)*$"
	FSType "zfs"
	LogOnce true
</Plugin>

<Plugin python>
	ModulePath "/usr/local/lib/collectd_pyplugins"
	LogTraces true
	Interactive false
	Import "disktemp"

	<Module "disktemp">
	</Module>
</Plugin>
% if graphite:

LoadPlugin write_graphite
<Plugin "write_graphite">
	<Node "graphite">
		Host "${graphite}"
		Port "2003"
		Protocol "tcp"
		LogSendErrors true
		Prefix "servers."
		Postfix ""
		StoreRates true
		AlwaysAppendDS false
		EscapeCharacter "_"
	</Node>
</Plugin>
% endif
